### Lawrence M. Friedman, Work Accidents: A Drama in Three Acts, 40 Hofstra Lab. & Emp. L.J. 463 (2023)

#### Act One: The Fellow-Servant Rule

In the early 1840's, Nicholas Farwell brought a lawsuit against his employer, the Boston & Worcester Rail Road. Farwell was an "engine-man" who earned two dollars a day. One day in 1837, he suffered an accident, was thrown to the ground, and the wheels of a railroad car passed over his right hand and crushed it. The accident was caused (or so Farwell alleged) by the carelessness of other employees.

The highest court of Massachusetts turned down Farwell's claim. The Chief Justice of Massachusetts, Lemuel Shaw, wrote the opinion. Shaw was an important figure in the legal world of the nineteenth century; he would probably be surprised to learn that his fame today, such as it is, is mostly due to the fact that his son-in-law was the author Herman Melville. Shaw was a very able judge, however, and the *Farwell* case was one of his most notable and significant decisions.

In any event, Nicholas Farwell's lawsuit failed; and he was left without any money from the railroad. His life, even before the accident, was surely very different from the comfortable life of the Chief Justice of Massachusetts. I have no idea what Farwell's life was like, after he lost his case. Did he have a wife and children? How did the family manage, after the accident? The crushed hand probably made it hard for him to earn a living. Today, every legal historian knows the case and recognizes Farwell's name. But at the time, this kind of recognition would have been cold comfort to Farwell and his family.

Farwell's case, as we said, rested on the claim that a fellow worker was negligent; and that this negligence caused the accident. It was not an implausible claim. Normally, an employer (a "principal") is responsible for the careless acts of an employee (an "agent"), provided these acts were done as part of the employee's job. If a negligent employee of the railroad had caused an accident which injured a person who had been walking near the tracks, the victim could have sued the railroad as the "principal" whose agent had been responsible for the plaintiff's injury. But Shaw refused to apply that rule in Farwell's case. One or two earlier cases seemed to be on Shaw's side. The most important one was an English case, *Priestley v. Fowler,* though that case came out of a different context; and might have had a somewhat different meaning. Technically of course, the English case, and the sole American case on the subject, were hardly binding on the Massachusetts court. *Farwell* itself was not binding on the law of any other state; the case did, however, prove to be persuasive, or perhaps infectious. It helped establish in the United States, the so-called fellow-servant rule The employer or a worker is not liable in tort, if the injury of a worker came about because of the carelessness of another worker (a "fellow servant") In practice, this shut off any chance of collecting damages in the typical case of an industrial accident. If a train, because of some sort of negligence, went off the tracks, the passengers could sue (and win, if they could show negligence), merchants who shipped goods could collect for any damage to the shipment--but a worker, who was collecting tickets in the railroad cars, had no hope of successfully suing the railroad.

*Farwell* is also considered a leading case on the doctrine of assumption of risk: roughly, the notion that workers like Nicholas Farwell know what they are getting into, know the risks, and choose to take the job nonetheless. Shaw makes this very point. Farwell's wages, moreover, supposedly reflected the fact that the job was dangerous. His pay scale was assumed to take the risks into account. At least that is what Shaw implied in his opinion.

In any event, the fellow-servant rule, as Shaw formulated it, proved persuasive to other courts. Under American law, the states are legally sovereign, at least in matters of ordinary tort and contract law. No court was obliged to follow *Farwell;* it was Massachusetts law and had no force outside of the state's borders. Yet almost every American court did adopt the fellow-servant rule. *Farwell*, then was not an idiosyncratic, one-off decision. It was a doctrinal move that other judges found convincing. To be sure, *Farwell* was a well-written, closely reasoned case by a leading and highly respected judge. No doubt this made some difference. But the fellow-servant rule would probably have made its way, even without the *Farwell* case, even without Shaw's well-crafted opinion.

What made the fellow-servant rule so attractive to the judges in the generation *after Farwell?* There were probably a number of reasons; and scholars do not agree on this point. Shaw's opinion gives, I believe, some clues. In one passage in the opinion, Shaw, referring to Farwell's injury, calls it a "loss \[that\] must be deemed to be the result of a pure accident, like those to which all men, in all employments, and at all times, are more or less exposed" Like "similar losses from accidental causes, it must rest where it first fell." 

I think the key word here is "accident;" or perhaps the phrase, "pure accident" "Accident" was and is, of course, a common English word, but its meaning has changed subtly over time. People still have accidents, even "pure" accidents; the social and cultural consequences, however, are not what they were in Shaw's day. Nowadays, losses from "accidents"--car crashes, for example, or botched medical procedures, or even flood or hurricane damage--do not necessarily "rest" where they "first fell" Of course, in the first half of the nineteenth century, there was no welfare state, no FEMA, no unemployment insurance: hardly any insurance at all. Medicine was crude and ineffective. Ships sank, locusts ate crops, banks failed and dragged down their depositors: life was full of calamities, and most of them were considered "pure accidents;" they did in fact rest where they first fell. People were surely aware of all this, and had to accept it, whether they liked it or not. Farwell's fate was the fate of many other men who lived when he did, and who suffered from the lottery of life.

The fellow-servant rule definitely tilted the scales against workers. It favored enterprise. It put the loss where it "first fell;" that is, on Farwell, not on the railroad. Was this a conscious and deliberate choice on Shaw's part? Was he aware of the economic impact of his decision? Obviously, we have no way of knowing what Shaw had in mind. At the time of the *Farwell* case, this was still an agricultural country. Industrial development was in its infancy. The railroad business was young but growing fast. Railroads were genuinely popular with farmers, merchants, and probably people in general. The railroad meant growth, it meant prosperity, it meant a market for the farmer's goods. Many early cases that adopted the fellow-servant rule arose out of railroad accidents. The fellow-servant rule favored railroads over the men who worked on the railroad. In hindsight, the rule seems callous. But Shaw and his fellow-judges had no way to visualize the world of the late nineteenth century. They could not have foreseen the full toll of the industrial revolution, the appalling harvest of dead and disabled workers, the thousands of victims of accidents on railroads, in coal mines and factories, and on construction sites. They could not foresee that the railroad, society's darling in the decade *of Farwell* and the next twenty or so years, would become a hated institution, the octopus, with its tentacles strangling farmers and small merchants; that lay ahead.

The fellow-servant rule, of course, was not confined to railroads. And it became more and more important as industry grew in importance and in scale. The number of work accidents multiplied. In many instances, the fellow-servant rule blocked any chance the victim had to win damages. If there was negligence at all behind a work accident, it was likely to be the negligence of a fellow worker. At first, the courts stuck to the rule--a rule which they had, after all, invented. In time, however, the situation began to change. The sheer volume of accidents began to make a difference. Some judges, it seems clear, found the rule distasteful. Plaintiffs (or their lawyers) began to explore ways to avoid or modify the rule; they searched for holes or weaknesses in the doctrinal fabric. The rule itself was, in theory, fairly clear-cut. But it began to erode under the pressure of litigation. Not many lawsuits were successful; and there was an "enormous disproportion" between the number of lawsuits, win or lose, and the thousands and thousands of "work-related injuries and deaths" But signs of decay became gradually obvious. The "architecture of the classical law of torts," including the fellow-servant rule, began to lose "its coherence and form" The rule sprouted a whole host of exceptions; and exceptions to exceptions.

In a New Jersey case decided in 1865, a railroad bridge gave way when a "heavily loaded" train of cars went over the bridge, and a brake-man, Philip Harrison, was killed. The administrator of his estate claimed that the railroad company was "aware" that the bridge was "out of repair and was unsafe" The railroad demurred, no doubt on the basis of the fellow-servant rule. But the high court of New Jersey overruled the demurrer. The case paid lip-service to the fellow-servant rule, said it was "completely established," cited *Farwell*, but then went on to say that the rule did not apply to the negligence of the "master," rather than the negligence of a fellow-employee; the poor condition of the bridge was, presumably, the fault of the company as a whole, and therefore of the "master," and not one of his agents. Out of cases like this, there developed the so-called "vice-principal" rule; the fellow-servant rule does not apply if the negligence comes from someone in authority in the company, a "person who so far represents the principal that the law charges the principal with liability for the acts ... of such person" In one case, for example, decided by the United States Supreme Court in 1884, the injured workman was an engineer, whose injury was caused by the "gross negligence" of the conductor of the train. The conductor, who was in charge, was (in the Court's view) "the personal representative of the corporation," and was therefore not really a fellow-servant at all. This doctrine blew a fairly big hole in the fellow-servant rule. So did a related rule which required the master to provide a "reasonably safe place to work in, and reasonably safe appliances with which to do the work" 

By the end of the century, even though the fellow servant rule was still a major obstacle, tort cases, some of them arising out of industrial accidents, were common in the courts. A certain number of plaintiffs who had suffered injuries at work were actually winning their cases. A steel company in Cleveland, Ohio, according to one study, lost every single lawsuit brought by an injured worker, between 1898 and 2015. Legislatures, too, had begun to chip away at the rule. A number of states, in the late nineteenth century, modified or abolished the rule, either for railroads or more generally. Kansas, for example, enacted a statute in 1874 which made railroads in that state "liable for all damages done to any employee ... in consequence of any negligence" of the railroad's agents. Iowa had a similar statute, and laws of this sort were common by the early twentieth century: laws applying to railroad accidents, or accidents in mines, and (in a few instances) applying more broadly. 

#### Act Two: Worker's Compensation Arrives

In the early twentieth century, in other words, there was increasing pressure on the fellow-servant rule. It became hard to ignore the appalling harvest of industrial accidents. In 1910, Crystal Eastman, supported by the Russell Sage Foundation, published a study of work accidents in Pittsburgh, part of a general "Pittsburgh survey" in six volumes. Eastman in her book, made more vivid with stark photographs of workers and work conditions, laid out clearly and in detail the horrendous nature of work accidents, the crushing impact of these accidents on workers and their families, and the pitiful amounts that workers themselves and their widows and children were generally able to collect. Out of a group of 120 instances in which a married worker was killed, a quarter of the families got nothing at all; and more than half got \$100 or less. Some employers were beginning to realize the "injustice of leaving the whole burden of accidents upon the workmen," and were providing some relief to the injured and their families. But other employers did nothing.

By this time, however, the fellow-servant rule was in trouble. It had been battered by court decisions. Piecemeal legislation had made some inroads. It had been subjected to withering criticism. Change had come about even before Eastman published her book. The Federal Employers' Liability Act (hereinafter "FELA") did away with the fellow-servant rule for railroad workers; the statute also cut back the defense of contributory negligence The United States Supreme Court, in a narrow and technical decision, struck down this statute; but congress amended the law in 1908; and the Supreme Court upheld this version of the law. Under FELA, railroads were liable for any deaths or injuries resulting from the negligence of employees. The Jones Act of 1920 provided that a seaman injured or killed "in the course of employment" had the same rights as against an employer as railroad workers. These were, of course, federal laws, and applied only to interstate and admiralty workers. They essentially retained the common law tort system, but stripped away the more egregious rules, including the fellow-servant rule, that blocked injured workers from suing their employers successfully.

In the states, a somewhat more radical path was followed: the enactment of workers' compensation law, a system which essentially guaranteed recovery for industrial accidents. The general idea had been discussed for years. A British compensation law had become law in 1897; a German plan had been adopted even earlier. Many European countries worked out plans along more or less similar lines. The idea, in short, was more or less in the air. Crystal Eastman's Pittsburgh study came out strongly in favor of "a law requiring employers to compensate all employees injured according to a uniform method;" a law of this type, covering industrial accidents, would do away with the slow, inadequate, unjust and "wasteful" system of tort law 

In fact, such a law soon became a reality in the states. Workers' compensation laws replaced the fellow-servant rule, along with other rules that had made tort law so stingy, so unpredictable, and so deficient in social justice. Workers' compensation, in essence, was a no-fault system for victims of industrial accidents. The basic idea was simple: a person injured on the job would be entitled to compensation. The fellow-servant rule was abolished. So was the defense of contributory negligence. A tired and overworked employee, who let his guard slip, and was injured, no longer lost any hope of compensation.

But there were obstacles in the way: opposition to the idea in general, and with regard to certain details. Unlike Great Britain, where courts had no power of judicial review (for better or for worse), courts in the United States had such a power; they could declare statutes unconstitutional; and in the late nineteenth and early twentieth century, they were hardly bashful about using this power. Any piece of major legislation had to run the gamut of judicial review. New York was the first state to pass a compensation law, in 1910, but the top court in New York struck the statute down in 1911. This, however, turned out to be only a temporary setback. The New York Constitution was amended to meet the courts' objections. In other states, the statutes had less trouble in court. The United States Supreme Court also rejected objections to compensation statutes, in a case decided in 1917. In the decade between 1910 and 1920, workers' compensation laws were enacted in most of the states. States of the deep south held out for a while; Mississippi was the last; and it fell in line in 1948. To be sure, in every state, passage was something of a struggle, if not on the principle itself, then on the form. But, by this time, it was a striking fact that management, as well as labor, was more or less on board in many of the states. In Wisconsin, for example, "large employers throughout the state lined up solidly behind the workmen's compensation bill;" and the legislature adopted a law in 1911 by a "lopsided" vote. In 1910, a survey conducted by the National Association of Manufacturers found that 95% of the businesses surveyed supported some sort of compensation plan. And in 1911, the National Association of Manufacturers "fully endorsed workers' compensation" 

At least with the benefit of hindsight, the general idea of workers' compensation had become more or less inevitable; the rush to enact these laws in the decade after 1910 is a kind of proof of this notion. To be sure, no two statutes were identical. In broad outline, however, they all conformed to a pattern. The laws established a plan which, in essence, was a rough compromise between the interests of employers and workers. The workers got the right to recover for injuries on the job, with very few exceptions. The laws wiped out the fellow-servant rule, and the defense of contributory negligence. In exchange, labor gave up its right to sue in tort; under the law, workmen's compensation was the workers' exclusive remedy. An injured worker had to settle for whatever the compensation statute offered. Recovery was more or less certain--but it was also limited.

Workers' compensation blasted a huge hole in the traditional tort system. The appalling toll of accidental deaths was perhaps the crucial political factor. Also, the fellow-servant rule had become unwieldy as well as socially unsatisfying. John Witt has also connected the compensation movement with the dramatic rise of statistical knowledge. To Chief Justice Lemuel Shaw, as we saw, an "accident" was a kind of random and unpredictable event; like being struck by lightning. In his day, no one was counting and measuring accidents. But by the late nineteenth century, it was possible to think of industrial accidents "in terms of probabilities" In many kinds of business--railroading, mining, construction--accidents were bound to happen. The same was true for all major projects. If a company had a contract to build a big bridge, or a skyscraper, or if a company planned to open a new coal mine, the company could reckon on a certain number of accidents, a certain number of injuries and deaths. There was no way to predict *which* workers would die or be injured. But you could gage, more or less, how many deaths and injuries were bound to happen. Injuries on the job, then, became in a way "nobody's fault in the personal sense" Each particular injury might be the result of a pure accident. But in the aggregate, deaths and injuries were an inevitable part of ordinary business; they were "attributable to the inherent hazards of industry" 

John Witt's insight helps explain the rise of workers' compensation. It connects the compensation movement to the rise of what has been called the metric society Data, figures, and statistics made the dimensions of the problem clear; and cast the problem in a distinct new light. For many kinds of business--coal mines, railroads, and construction companies, for example-- accidents were inevitable, but measurable; they could therefore be anticipated, and thus at least potentially controlled and contained The big change was not only in techniques of measurement, but also (and perhaps more profoundly) in the culture of risk. This was becoming, more and more, an insuring and insurable society. The insurance business grew enormously in the latter part of the nineteenth century It is profitable precisely because of its ability to predict and measure risk.

The nineteenth century was an age of science and technology. It was the century of the railroad, the telegraph, the telephone; later in the century came the wireless; and the first steps toward the development of the automobile. It was also the century of Darwin. It was an age of innovation and improvement in manufacturing. All of this had a profound impact on culture. New technologies and new discoveries changed the way people looked at the world around them. As people came to understand natural forces, as science marched on, the culture changed. Knowledge affected the level of social demands: demands to *do* something. Nothing, of course, could be done about the earth's orbit or the phases of the moon; but accidents and diseases were another matter. Science injected into society new ideas about causes of diseases, catastrophes, and "accidents" Science was the enemy of fatalism. It was the enemy of half-baked ideas about causes and effects. What, for example, explained the outbreak of epidemics? Was it foul air? Was it divine retribution for sin? In the nineteenth century, the development of the "germ theory" put forward a new and different answer Cholera, for example, was no longer a mysterious killer that exploded into a community without warning. Microscopic creatures ("germs") were polluting the water supply: that was the source of cholera epidemics As early as 1854, a British scientist, John Snow, became aware of the role of bad water Londoners who used one water pump died in droves; those who used water from a cleaner source did not Adding chlorine to the water supply gave society control over cholera Understanding that polluted water caused cholera, and that "germs" were the source of the pollution, changed attitudes toward the disease. This led to demands for improvements in sanitation, in public health; chlorine could conquer the "germs" Much could be done; but only government could do it.

A somewhat similar dynamic helped move the compensation movement forward. Of course, the sheer volume of industrial accidents lent force to the politics of change. But a strong belief in progress, reform, and a greater emphasis on research, on measurement, may have been even more important. In any event, by the beginning of the twentieth century, it seemed clear that the traditional tort system was seriously broken. And there was wider and wider agreement that something could be done to fix it.

The first workers' compensation laws, however, were in many ways weaker and more tentative than the compensation laws of today. Kansas, for example, enacted a compensation law in 1911. It was, in the first place, elective: both employers and employees could decide to come under the law; or remain outside. The law applied only to employers with 15 or more employees and it applied only to accidents in dangerous occupations--manufacturing, mining, quarrying, work at natural gas plants for example--along with jobs that made use of dangerous explosives or inflammable materials. There were similar provisions in the Illinois law of 1911, which listed occupations which were "especially dangerous," like quarrying, and work with "explosive materials," posing "extraordinary risks to life and limb" The statute in Washington state (1911) applied to all "inherently hazardous works and occupations" Not all of the statutes were limited in this way: the California law, for example, also enacted in 1911, was elective (like the Kansas law), but it applied to industry in general. The Ohio law, also of 1911, was another elective law; both companies and workers could opt out. But within a few years, the legislature amended the law, and workers' compensation became mandatory for employers. This soon became the norm in state after state.

#### Act Three: The Strange Career of Workers' Compensation

By 1920, most states had adopted a system of compensation; there were a few laggards (mostly in the South), but these too soon fell into line. The last state, Mississippi, enacted its law in 1948 The state laws were hardly static. They were in a constant process of change. In general, the law of workers' compensation evolved along two parallel tracks: a legislative track, steadily expanding coverage; and a judicial track, case-law interpreting the statutes in a more and more inclusive way.

The compensation laws had grown out of the dark world described by Crystal Eastman: a grim and perilous world, the world of coal mines, construction sites, railroad yards, and factories full of whirling, dangerous machines. That world was populated with dead workers and destitute families: victims of jobs that could maim and kill. Some of the early statutes, as we noted, were specifically limited to dangerous jobs and dangerous work situations. That did not last. The statutes no longer applied only to inherently dangerous jobs. In a Kansas case, decided in 1923, a passenger on a street railway stabbed the conductor, Orville Stark, to death. This was hardly the usual sort of industrial accident. Was it compensable? The court said yes. Stark's death was the result of "a hazard incident to employment as a street-car conductor" The conductor collects money; and might therefore be the target of robbers. He could also be assaulted by drunks, "brawlers," and people with a "mental twist or unbalance" Because of his job, the conductor is "exposed" to these dangers. These incidents, to be sure, would be "infrequent;" but they came from risks inherent in the job. Stark's family was entitled to death benefits under the workers' compensation law.

Of course, it would be something of a stretch to call a conductor's job dangerous; certainly not as compared to coal miners or factory workers. But stretches of this kind were more and more the norm in compensation cases. In an Idaho case from 1947, *Louie v. Bamboo Gardens*, the claimant, Tom Louie, 45 years old, was a dishwasher in a restaurant in Boise, Idaho. A crazed gunman burst into the restaurant; Louie was carrying water glasses from the kitchen into the main part of the restaurant; the gunman shot his loaded revolver three times; one of the bullets struck Louie "in the upper back region, piercing his chest cavity and his lungs" The Industrial Accident Board denied Louie's claim; but the Supreme Court of Idaho reversed. The "modern tendency of the decisions," the court said, and the "spirit of the law," meant that compensation was to be awarded "in all cases where a liberal construction of the statute would justify it" The Idaho court cited, among other cases, a New York decision, *Leonbruno v. Champlain Silk Mills* (1920). The claimant, working in the silk mill, "was struck by an apple which one of his fellow servants, a boy, was throwing in sport at another, and as a consequence \[Leonbruno\] lost the better part of the sight of one eye" Leonbruno was entitled to compensation. And so was Tom Louie.

Judges clearly took the notion of "liberal" construction of the law quite seriously. Crazed gunman, tossed apples: these were hardly "industrial accidents" in the usual sense of the phrase. Judges, however, were hardly alone in pushing the envelope. The statutes, too, became steadily broader. No longer did the statutes, explicitly or implicitly, confine themselves to the ordinary human wreckage of the industrial revolution. In New York, the legislature kept adding to the list of hazardous occupations; then, in 1918, the legislature labeled as hazardous "all employments with four or more workmen," on the theory that "any employment in which an accident took place was, ipso facto, a dangerous employment" In short, almost *any* work accident--anything that happened "in the course of employment" entitled a worker to compensation. 

The 1918 New York law implied a real expansion of the law: that any job where an accident took place was to that extent a dangerous job. Even a bland and boring office job could be "dangerous," if in fact a worker was injured on the job--by slipping and falling on the stairs, for example. Accidents are compensable when they "arise" out of the employment. The meaning of this word has expanded like a balloon over the years. At first, compensation followed only if the job posed some peculiar risk. But more and more, over time, courts edged toward what was called the "positional-risk doctrine:" an assumption like the one underlying the case of Tom Louie. The mere fact that the work caused you to be in a certain place, and exposes you to a risk, even a tiny or weird or unusual risk, raises a presumption that you have a right to compensation. And if the injury is mysterious: a worker suddenly falls and breaks a leg at work, for no obvious reason, the same presumption holds. Not all courts go along with such notions, but mostly they do. This trend in compensation law ran parallel to trends in tort law in general. This was the age of the so-called "liability explosion" Kenneth Abraham has spoken of a "transformation" in the very concept of negligence, the key concept in tort law--a transformation "far beyond what anyone living at the turn of the twentieth century could have predicted" In Benjamin Cardozo's famous decision in the New York Court of Appeals, *MacPherson v. Buick Motors*, MacPherson bought a Buick; he was injured when a defective wheel fell off the car. MacPherson sued the Buick motor company. What stood in the way was the so-called privity doctrine. MacPherson bought the car from a dealer, not from Buick. Under the privity doctrine, he should have sued the dealer, with whom he was in "privity," rather than the manufacturer. This was, indeed, the law, but there were exceptions-- for products which were, for example, imminently dangerous. The privity doctrine had already shown signs of decay; courts seemed to pin the "dangerous" label on more and more products. Cardozo gave the law a subtle twist: he expanded the category of "dangerous" products in such a way as to include absolutely everything--if a pillow or a handkerchief caused an injury, it was to that extent a dangerous product. This meant that privity was dead. A consumer, injured by a product, could ignore "privity" and sue the actual manufacturer. A whole new field, product liability, flowed from this case, and the cases that followed it. 

In compensation law, too, courts and legislatures expanded coverage far beyond anything the proponents had dreamt of. The laws typically defined compensable injuries as those "arising out of and in the course of" employment. The basic meaning of this phrase was clear: injuries had to take place at work and had to be related to work. A worker is hurt, sliding into third base at a company picnic. Is this compensable? Is it "in the course of" employment. Possibly yes. In a 1969 case in New York, a "33-year-old time study engineer suddenly collapsed and died while playing ball at the annual picnic of the Management Club of the New York Air Brake Company" The Workmen's Compensation Board granted the widow's claim for a death benefit; New York's courts affirmed. The company was intricately involved in this social event; thus, the worker's death was "within the scope of employment." Company picnics, Christmas parties, and the like have figured in dozens of lawsuits. Claimants did not always win; but many did. Courts took the issue seriously. What about heart attacks on the job? Courts struggled with this issue for a long time; there were distinctions between "usual" and "unusual exertion" in some states. The case law was tangled; decisions sometimes depended on the precise wording of a statute; but overall, employees had considerable success in winning these heart attack cases. On the other hand, if a worker slips and falls in the restroom at work, while answering the "call of nature," no court would deny compensation. Arguably, going to the bathroom is not part of a person's job; but any such argument will simply not work. There are also special rules for traveling salespeople: they are "in the course of" employment twenty-four hours a day. If a salesperson, spending the night in a motel, falls out of bed, the salesperson has a clear claim for compensation. If a fire breaks out in the hotel, and the employee is injured, that too will be covered. And when an employee of the Home Rubber Co., in charge of European sales, books passage for Europe, and has the bad luck to be traveling on the Lusitania, sunk by German torpedoes, he too is covered. 

The march toward more and more coverage continued over the years. Perhaps the biggest and most important change has been the move to include occupational diseases. The early statutes stressed "accidents;" the California statute of 1911, for example, provided for injuries "accidentally sustained" or caused by "accident." Occupational diseases do not fit such terms easily; "accident" seems to refer to a single traumatic event, which of course was the image that lay behind the original statutes. Occupational diseases often develop slowly, when workers are exposed to pollutants, or to work conditions which after a while hurt or sicken the worker. The early statutes did not cover occupational diseases at all; since they were not "accidents" or the result of "accidents" The legislatures changed this situation, at first by listing particular diseases, which were clearly "occupational" Eventually, the statutes were expanded to cover *any* disease that was work-related. By 1978, every state had some sort of coverage for occupational diseases. In Pennsylvania, for example, there was a long list of diseases in the statute, including anthrax, a disease affecting workers who handled "wool, hides ... or bodies of animals;" but the statute also came to include a catch-all clause: any disease which claimants were exposed to on the job; provided the disease was more common for workers in that job, compared to its incidence in the population at large. The current Missouri statute allows claims for any "identifiable disease arising ... out and in the course of the employment" so long as "occupational exposure was the prevailing factor" in causing the condition. 

The movement to expand coverage has not gone entirely in one direction. Farm workers and domestic workers were often not covered (in Indiana, for example), even though these might be the workers who need compensation the most. The issue of mental health has also been troublesome. A worker is fired, or transferred, or demoted. The worker claims this led to some sort of nervous breakdown. Can the worker collect compensation? In a 1985 Massachusetts case, a supervisor had told Helen J. Kelly that her company was downsizing; that her job had been eliminated. She burst into tears. The company, perhaps out of sympathy, kept her on, but transferred her to another job. Kelly continued to be extremely upset; she "became depressed, developed chest pains," had to be hospitalized, and ended up totally unable to work. The court ruled that she was entitled to compensation. But claims of this sort, particularly for bitter workers who had been fired, spiraled upwards. It was easy, after all, for a person to blame psychological or emotional injury on heartless bosses, who certainly do exist. On this issue, faced with mounting premium costs, the employers fought back. Some states, giving in to the pressure, amended its law to get rid of such claims. In California, for example, if a "psychiatric injury" is the result of a "lawful, nondiscriminatory, good faith personnel action," there is no right to compensation. If Helen Kelly brought her case in California, in today's world, she would probably lose.

There have been other examples of backtracking. In Texas, employers can opt out of workers' compensation; and so can employees. But Texas law is exceptional here. There does not seem to be a major move to go back to the bad old days. This is not to say that there is universal satisfaction with the system: that workers get enough money out of workers' compensation; or that instances of rank injustice never occur. The laws take no account of pain and suffering. A case can be made that the system is on the brink of failure-- at least as a way of making injured employees whole. Compensation law is, in a way, essentially heartless; arguably, it treats workers as commodities, not as suffering human beings. The treatment of occupational disease is particularly unsatisfactory--for example, during the recent COVID-19 pandemic. Employers have their own set of complaints; some have mobilized to try to lower the costs of workers' compensation, or to persuade states to join Texas in making the system optional. Nonetheless, the system seems firmly entrenched, even if it is far from adequate. And the history of compensation law, despite some zigs and zags, is plainly a history of constant expansion, at least in the scope of coverage. Heart attacks, accidents at company picnics, damages for breathing foul air: these types of claims are not always successful; but it would have surprised the men and women who, appalled by the grim toll of industrial accidents, fought for the first compensation laws.

How can we explain the strange career of workers' compensation? Or, for that matter, how can we explain the career of tort law in general, in the twentieth century--the so-called liability explosion? The liability explosion is a complex story, and I will leave it to other people to try to explain. But I would like to make a point about a facet of American political and economic life which is, I think, a small part of the explanation of the liability explosion, and a large part of the explanation of why workers' compensation expanded so dramatically. The fact is that there is a gaping hole at the heart of the American welfare state. In the United States, there is no general, universal system of health insurance, no national plan on the model of the British or Canadian plans; or the plans in every other country that is part of the developed world. A country like Canada--or France, or Germany--that has some sort of plan, supported by tax-payer money, to provide free health care for everybody, is a country which does not particularly need a special scheme covering workplace accidents. Many of these countries once had a form of workers' compensation. In many of them, it still exists--in some form. It may put the cost of work accidents on the employer, instead of the taxpayer. But as far as the worker is concerned, this hardly matters. It may make no practical difference to the poor soul with broken bones who slipped and fell at home on Sunday, compared to the one who slipped and fell at the office on Monday; or who suffered a heart attack while watching a movie, as opposed to an attack that happened while the worker was moving furniture at work, or operating a machine on the factory floor. In the United States it does make a difference, and often a crucial one.

This gap in the welfare system has made its mark on the tort system as a whole. That system, as John Witt has pointed out, is a "patchwork;" a "hodge podge of systems," an "accidental product" These phrases fit worker's compensation as well; and (very notably) the American health system (or lack of one). There is no universal, national scheme of free health care, as we said. But there is Medicare for old people and Medicaid for poor people. There is "Obamacare" There are health "benefits" through the job, for millions of workers. Veterans are entitled to medical care at VA institutions. Congressmen have free medical care. There are special provisions for kidney dialysis, under a law passed by Congress in 1972. And then there is workers' compensation: another small patch in the quilt.

Developed countries (other than the United States) all have, as we said, a national health system of one sort or another. That they all have such a system is not a historical accident, any more than the fact that they all provide free public school; or that women vote; or that they have adopted liberal divorce laws. They have all responded to a cluster of economic, political, and cultural factors, which are common to the developed world (and the developed parts of the undeveloped world). In many regards, the United States has gone along with global trends. There is Social Security, and no-fault divorce, and free public education. There are dozens and dozens of health and safety laws. There is deposit insurance and unemployment compensation. But with regard to health care, the country is still something of an outlier. For a variety of reasons, health care remains a complex and controversial issue. America has no national health system, on the British or Canadian model. Yet trends in health care have gone, essentially, in only one direction: edging closer toward a universal system, but never quite getting there (so far). Underlying this trend is a change in the culture: much or most of the public would like such a system; more and more people feel that health care is or should be a right, like public school education. More and more people feel health care should not be left to the whims and winds of the market. It is not easy to say exactly what accounts for this cultural change. Or whether it is likely to continue, to grow, or stay as it is. But the trend, so far, seems clear.

#### Conclusion

A few final comments on this three-act drama. First of all, it illustrates an old but obvious fact: legal change is a function of social change; or perhaps it might be better to say, a function of social context. The fellow servant rule was born in the early railroad days, the early days of industry. The shift to an industrial society in the end destroyed it: the terrible toll of industrial accidents made it politically toxic, and, in the end, the old rules no longer worked. Workers' compensation was a way to solve the problem of the costs of industrial accidents, both for labor and for management. It did solve the problem, to a degree. The system was then caught up in our third act; and it expanded, to do its bit to fill the gaping hole in the health care system.

Is a fourth act possible? Yes: possible and even likely. The structure of the work force seems to be changing. Thousands of workers are part of the so-called gig economy. Millions of people are no longer "employees;" they need health insurance (which can cost them dearly). For millions of people, the cost of medicine and drugs can even push them into bankruptcy. Millions are still without any health insurance. Health care is or can be a major issue. A national system might develop. In that case, it would no longer matter very much, where and when an accident occurred. A broken leg would be a broken leg, any time and any place. The United States would march in step with the rest of the developed world. Or, conceivably, time could flow backward: back to the days when there was no relief for "pure accidents" This seems less likely. But the future is, as always, dark and unknown.